{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec052c0-3bb2-466b-a7e0-c74460e858e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d312f-b1b0-438a-8755-20932c882c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting working directory\n",
    "work_dir = \"/storage/work/~~~\" \n",
    "os.environ['HF_HOME'] = f\"{work_dir}/.cache/huggingface\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = f\"{work_dir}/.cache/huggingface\"\n",
    "\n",
    "# Huggingface token\n",
    "hf_token = \"hf_~~~\" \n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c3a838-0dff-45b8-95a5-052e27106d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/work/jkc6529/.conda/envs/llm_deployment/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b6ec5c-1fc5-4f75-95bf-a742ba3ebb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/work/jkc6529/.conda/envs/llm_deployment/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/storage/work/jkc6529/.conda/envs/llm_deployment/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6f67508fb644eca5f38377aebf6639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cef630cd43a422ba974b537bb7f4867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "### Llama-3\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    tokenizer_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    context_window = 8192,\n",
    "    max_new_tokens = 256,\n",
    "    generate_kwargs = {\"temperature\": 0.1, \"do_sample\": False},\n",
    "    device_map = \"auto\",\n",
    "    model_kwargs = {\"torch_dtype\": torch.float16, \"load_in_8bit\": False} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4e1273-c29e-428a-8955-e8ecf9be3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading documents\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\".\", \n",
    "    required_exts=[\".pdf\", \".docx\"] \n",
    ")\n",
    "documents = reader.load_data()\n",
    "\n",
    "# Creating index and engine\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b679d2d-96f5-4c76-96fc-05906e25ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is difference between prompt sensitivity and model sensitivity?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d732891-0c0d-4829-937b-ad16c04855d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/work/jkc6529/.conda/envs/llm_deployment/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/storage/work/jkc6529/.conda/envs/llm_deployment/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking Question: What is difference between prompt sensitivity and model sensitivity?\n",
      "\n",
      "==========\n",
      "Response:\n",
      " Prompt sensitivity refers to the variation in metrics across different prompt templates or phrasings, whereas model sensitivity refers to the comparison of different models (e.g., GPT, Llama, Deepseek, etc.) and examining where they disagree with humans and with each other. In other words, prompt sensitivity is about the impact of different prompts on the model's performance, while model sensitivity is about the differences in performance between different models.\n",
      "==========\n",
      "Citations:\n",
      "- Page 8: 8/13\n",
      "Step 3-Stability Evaluation:Reduce cross-prompt/model variance\n",
      "Prompt Sensitivity\n",
      "- Test multip...\n",
      "- Page 2: 2/5\n",
      "Applications- GenAI vs. Human Fact-checker (Tai et al., 2025)\n",
      "Models:\n",
      "- GPT-4o (OpenAI)\n",
      "- Llama ...\n"
     ]
    }
   ],
   "source": [
    "# Input (question)\n",
    "print(f\"Asking Question: {question}\")\n",
    "response = query_engine.query(question)\n",
    "\n",
    "# Output (answer)\n",
    "print(\"\\n\" + \"=\" * 10)\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "print(\"=\" * 10)\n",
    "\n",
    "# Source\n",
    "print(\"Citations:\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"- Page {node.metadata['page_label']}: {node.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b69b822-d2f4-4dc1-8ff7-e948bba89a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Without RAG]:\n",
      " Cassandra Tai is a popular American social media influencer, content creator, and entrepreneur. She is known for her lifestyle, beauty, and fashion content on platforms like Instagram, TikTok, and YouTube.\n",
      "Cassandra Tai was born on August 24, 1995, in the United States. She grew up in a loving family and was raised with a strong sense of values and morals. From a young age, Cassandra was fascinated by the world of beauty and fashion, and she spent hours watching makeup tutorials and fashion shows on TV.\n",
      "After completing her high school education, Cassandra decided to pursue a career in the beauty and fashion industry. She started by working as a makeup artist and hairstylist, and she quickly gained a reputation for her skills and attention to detail.\n",
      "In 2015, Cassandra created her Instagram account, where she began sharing her passion for beauty, fashion, and lifestyle. Her account quickly gained popularity, and she soon became known as a social media influencer. She used her platform to share her favorite products, trends, and tips with her followers, and she quickly built a loyal community of fans.\n",
      "In addition to her Instagram account, Cassandra also created a YouTube channel, where she shares makeup tutorials, product reviews, and other beauty-related content. She has\n",
      "\n",
      "[Using RAG]:\n",
      " According to the context information, Cassandra Tai is the author of the document \"LLM_SSRI.pdf\" and is affiliated with the Center for Social Data Analytics (C-SoDA) at Penn State University. She is also the speaker at the Social Science Research Institute Open House.  She has published a research paper titled \"Applications- GenAI vs. Human Fact-checker (Tai et al., 2025)\" with co-authors.  However, the context does not provide information about her role or position at Penn State University.  It only mentions that she is the speaker at the Social Science Research Institute Open House.  Therefore, it is not possible to determine her role or position at Penn State University based on the provided context information.  It is also not possible to determine her full name as the context only provides her last name, \"Tai\".  It is possible that she is a researcher or an academic, but this cannot be confirmed based on the provided context information.  It is also possible that she is a student, but this cannot be confirmed based on the provided context information.  It is also possible that she is a staff member, but this cannot be confirmed based on the provided context information.  It is also possible that she is a visitor,\n"
     ]
    }
   ],
   "source": [
    "# Without RAG\n",
    "raw_response = Settings.llm.complete(\"Who is Cassandra Tai?\")\n",
    "print(f\"[Without RAG]:\\n{raw_response}\\n\")\n",
    "\n",
    "# Using RAG\n",
    "rag_response = query_engine.query(\"Who is Cassandra Tai?\")\n",
    "print(f\"[Using RAG]:\\n{rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf3f0e0-cc8e-41d9-9477-175c3158e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71eb801d-f7c9-4fc0-a1c4-be13e4cb04ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is difference between prompt sensitivity and model sensitivity?\n",
      "\n",
      "[score 0.3950] 8/13\n",
      "Step 3-Stability Evaluation:Reduce cross-prompt/model variance\n",
      "Prompt Sensitivity\n",
      "- Test multiple prompt templates / phrasings\n",
      "- Assess variation in metrics across prompts\n",
      "Model sensitivity\n",
      "- Compare models (GPT, Llama, Deepseek, etc\n",
      "[score 0.0754] Results: GenAI has potential but is fundamentally limited in its\n",
      "capacity to detect political content credibility\n",
      "[score 0.0730] )\n",
      "- Examine where models disagree with humans and with each other 9/13\n",
      "Step 4-Explainability & Oversight: Automated systems require oversight\n",
      "Use model-generated rationales as auditable artifacts\n",
      "Audit for:\n",
      "- Logical coherence and conceptual validity\n",
      "- Biases, hallucinations, ethical red flags\n",
      "Two-way humanâ€“AI collaboration\n",
      "- LLM rationales expand human awareness\n",
      "- Human experts correct, constrain, and document model behavior 10/13\n",
      "Step 5-Uncertainty & Error Correction:Acknowledging AI uncertainties\n",
      "Misclassification bias can distort regression and descriptive\n",
      "analyses\n",
      "Treat AI labels as noisy measurements, not ground truth\n",
      "Methods:\n",
      "- Design-based Supervised Learning (DSL): gold-standard subsample + inclusion\n",
      "probabilities (Egami et al\n"
     ]
    }
   ],
   "source": [
    "### Pure python retrieval\n",
    "doc_text = \" \".join([doc.text for doc in documents])\n",
    "corpus = [sent.strip() for sent in doc_text.split('.') if len(sent) > 20]\n",
    "\n",
    "# Question\n",
    "query = \"What is difference between prompt sensitivity and model sensitivity?\"\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus + [query])\n",
    "\n",
    "# Calculating cosine similarity\n",
    "similarities = cosine_similarity(X[-1], X[:-1])\n",
    "\n",
    "# Top 3 sentences\n",
    "top_k = 3\n",
    "top_indices = np.argsort(similarities[0])[-top_k:][::-1]\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "for idx in top_indices:\n",
    "    score = similarities[0][idx]\n",
    "    print(f\"[score {score:.4f}] {corpus[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc5c33c0-fc5e-4116-9205-cd91f28fa7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Answer (Top-k=1)]:  Unfortunately, the provided context information does not contain the entire document, but rather a snippet from the document. The snippet appears to be an outline or table of contents for a document titled \"Evaluation Pipeline: From annotation to oversight\" by Ko, Tai, and Webb Williams (2025). The outline mentions annotation, impersonating respondents, text annotation, and application cases, but does not provide any specific details about the methodology, results, or conclusion. Therefore, it is not possible to summarize the entire document based on this information. Further context or access to the full document would be necessary to provide a comprehensive summary.  If you have any additional context or would like me to assist with anything else, please let me know! \n",
      "\n",
      "[Answer (Top-k=5)]:  The document discusses the evaluation pipeline for large language models (LLMs) and the need for a systematic framework to validate and document their use. The authors propose a framework that includes annotation, impersonating respondents, and machine learning systems. They also discuss the importance of role-task mapping and the need for clear role definition to guide evaluation criteria.\n",
      "\n",
      "The authors present a case study on GenAI vs. Human Fact-checker, where they evaluate the ability of GenAI to assess content credibility. They use an archive of online communications from public officials and analyze over 6 million posts from Facebook and Twitter. The results show that GenAI can effectively assess content credibility, but the authors note that more research is needed to validate and document the use of LLMs.\n",
      "\n",
      "The methodology involves several steps, including annotation, impersonating respondents, and machine learning systems. The authors use a combination of zero-shot and few-shot learning to recover gold labels and evaluate the performance of the LLMs. They also use clustering tasks with zero-shot and without gold labels to evaluate the ability of the LLMs to generalize.\n",
      "\n",
      "The results show that GenAI can effectively assess content credibility, but the authors note that more research is needed to validate and document the use of LLMs. They conclude that a systematic framework\n"
     ]
    }
   ],
   "source": [
    "### Parameter tuning \n",
    "\n",
    "# top-k = 1\n",
    "engine_k1 = index.as_query_engine(similarity_top_k=1)\n",
    "response_k1 = engine_k1.query(\"Summarize the entire document including methodology, results, and conclusion.\")\n",
    "print(f\"[Answer (Top-k=1)]: {response_k1}\\n\")\n",
    "\n",
    "# top-k = 5\n",
    "engine_k5 = index.as_query_engine(similarity_top_k=5)\n",
    "response_k5 = engine_k5.query(\"Summarize the entire document including methodology, results, and conclusion.\")\n",
    "print(f\"[Answer (Top-k=5)]: {response_k5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f7b5009-83de-4101-adb2-f338a46aed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector dimension of question 'Social Science': 384\n",
      "First 10 elements of the vector: [0.061991237103939056, 0.046792175620794296, -0.027558419853448868, -0.035168781876564026, 0.009088116697967052, 0.008314818143844604, 0.0070055569522082806, 0.02457190304994583, 0.0032607668545097113, -0.011888917535543442]\n"
     ]
    }
   ],
   "source": [
    "### Vector Embeddings Visualization\n",
    "\n",
    "query_str = \"Social Science\"\n",
    "query_embedding = Settings.embed_model.get_query_embedding(query_str)\n",
    "\n",
    "print(f\"Vector dimension of question '{query_str}': {len(query_embedding)}\")\n",
    "print(f\"First 10 elements of the vector: {query_embedding[:10]}\")\n",
    "\n",
    "# These numbers collectively represent the semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52638f47-4324-449c-9554-a3da123d88bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd31882-d17b-4c1e-8fc8-1ce532905db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM Assignment)",
   "language": "python",
   "name": "llm_deployment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
