{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec052c0-3bb2-466b-a7e0-c74460e858e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d312f-b1b0-438a-8755-20932c882c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Setting working directory\n",
    "work_dir = \"/storage/work/~~~\" \n",
    "os.environ['HF_HOME'] = f\"{work_dir}/.cache/huggingface\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = f\"{work_dir}/.cache/huggingface\"\n",
    "\n",
    "# Huggingface token\n",
    "hf_token = \"hf_~~~\" \n",
    "login(token = hf_token)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40761b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3a838-0dff-45b8-95a5-052e27106d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6ec5c-1fc5-4f75-95bf-a742ba3ebb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    tokenizer_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    context_window = 8192,\n",
    "    max_new_tokens = 256,\n",
    "    generate_kwargs = {\"temperature\": 0.1, \"do_sample\": False},\n",
    "    device_map = \"auto\",\n",
    "    model_kwargs = {\"torch_dtype\": torch.float16, \"load_in_8bit\": False} \n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "Settings.embed_model = OllamaEmbedding(model_name = \"llama3\")\n",
    "\n",
    "Settings.llm = Ollama(\n",
    "    model = \"llama3\",\n",
    "    request_timeout = 360.0,\n",
    "    temperature = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e1273-c29e-428a-8955-e8ecf9be3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading documents\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\".\", \n",
    "    required_exts=[\".pdf\", \".docx\"] \n",
    ")\n",
    "documents = reader.load_data()\n",
    "\n",
    "# Creating index and engine\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b679d2d-96f5-4c76-96fc-05906e25ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is difference between prompt sensitivity and model sensitivity?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d732891-0c0d-4829-937b-ad16c04855d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (question)\n",
    "print(f\"Asking Question: {question}\")\n",
    "response = query_engine.query(question)\n",
    "\n",
    "# Output (answer)\n",
    "print(\"\\n\" + \"=\" * 10)\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "print(\"=\" * 10)\n",
    "\n",
    "# Source\n",
    "print(\"Citations:\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"- Page {node.metadata['page_label']}: {node.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69b822-d2f4-4dc1-8ff7-e948bba89a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without RAG\n",
    "raw_response = Settings.llm.complete(\"Who is Cassandra Tai?\")\n",
    "print(f\"[Without RAG]:\\n{raw_response}\\n\")\n",
    "\n",
    "# Using RAG\n",
    "rag_response = query_engine.query(\"Who is Cassandra Tai?\")\n",
    "print(f\"[Using RAG]:\\n{rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf3f0e0-cc8e-41d9-9477-175c3158e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb801d-f7c9-4fc0-a1c4-be13e4cb04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pure python retrieval\n",
    "doc_text = \" \".join([doc.text for doc in documents])\n",
    "corpus = [sent.strip() for sent in doc_text.split('.') if len(sent) > 20]\n",
    "\n",
    "# Question\n",
    "query = \"What is difference between prompt sensitivity and model sensitivity?\"\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus + [query])\n",
    "\n",
    "# Calculating cosine similarity\n",
    "similarities = cosine_similarity(X[-1], X[:-1])\n",
    "\n",
    "# Top 3 sentences\n",
    "top_k = 3\n",
    "top_indices = np.argsort(similarities[0])[-top_k:][::-1]\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "for idx in top_indices:\n",
    "    score = similarities[0][idx]\n",
    "    print(f\"[score {score:.4f}] {corpus[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c33c0-fc5e-4116-9205-cd91f28fa7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter tuning \n",
    "\n",
    "# top-k = 1\n",
    "engine_k1 = index.as_query_engine(similarity_top_k=1)\n",
    "response_k1 = engine_k1.query(\"Summarize the entire document including methodology, results, and conclusion.\")\n",
    "print(f\"[Answer (Top-k=1)]: {response_k1}\\n\")\n",
    "\n",
    "# top-k = 5\n",
    "engine_k5 = index.as_query_engine(similarity_top_k=5)\n",
    "response_k5 = engine_k5.query(\"Summarize the entire document including methodology, results, and conclusion.\")\n",
    "print(f\"[Answer (Top-k=5)]: {response_k5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b5009-83de-4101-adb2-f338a46aed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vector Embeddings Visualization\n",
    "\n",
    "query_str = \"Social Science\"\n",
    "query_embedding = Settings.embed_model.get_query_embedding(query_str)\n",
    "\n",
    "print(f\"Vector dimension of question '{query_str}': {len(query_embedding)}\")\n",
    "print(f\"First 10 elements of the vector: {query_embedding[:10]}\")\n",
    "\n",
    "# These numbers collectively represent the semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52638f47-4324-449c-9554-a3da123d88bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd31882-d17b-4c1e-8fc8-1ce532905db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
