{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec052c0-3bb2-466b-a7e0-c74460e858e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d312f-b1b0-438a-8755-20932c882c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Setting working directory\n",
    "work_dir = \"/storage/work/~~~\" \n",
    "os.environ['HF_HOME'] = f\"{work_dir}/.cache/huggingface\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = f\"{work_dir}/.cache/huggingface\"\n",
    "\n",
    "# Huggingface token\n",
    "hf_token = \"hf_~~~\" \n",
    "login(token = hf_token)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40761b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-ollama\n",
      "  Obtaining dependency information for llama-index-embeddings-ollama from https://files.pythonhosted.org/packages/af/53/9ab65a3d0db29f49967a292b7c33cf512e493e68cb63ef337b1fffadf489/llama_index_embeddings_ollama-0.8.6-py3-none-any.whl.metadata\n",
      "  Downloading llama_index_embeddings_ollama-0.8.6-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-embeddings-ollama) (0.14.13)\n",
      "Requirement already satisfied: ollama>=0.3.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-embeddings-ollama) (0.6.1)\n",
      "Collecting pytest-asyncio>=0.23.8 (from llama-index-embeddings-ollama)\n",
      "  Obtaining dependency information for pytest-asyncio>=0.23.8 from https://files.pythonhosted.org/packages/e5/35/f8b19922b6a25bc0880171a2f1a003eaeb93657475193ab516fd87cac9da/pytest_asyncio-1.3.0-py3-none-any.whl.metadata\n",
      "  Downloading pytest_asyncio-1.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.13.3)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.22.1)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.3.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2026.1.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.13.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\jooan\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.6.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.4.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (12.1.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\jooan\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (4.5.1)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.12.5)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (80.10.2)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.0.46)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.17.3)\n",
      "Collecting pytest<10,>=8.2 (from pytest-asyncio>=0.23.8->llama-index-embeddings-ollama)\n",
      "  Obtaining dependency information for pytest<10,>=8.2 from https://files.pythonhosted.org/packages/3b/ab/b3226f0bd7cdcf710fbede2b3548584366da3b19b5021e74f5bde2a8fa3f/pytest-9.0.2-py3-none-any.whl.metadata\n",
      "  Downloading pytest-9.0.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.22.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.15.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.1.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.16.0)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2026.1.15)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.4.2)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\jooan\\appdata\\roaming\\python\\python312\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (0.4.6)\n",
      "Collecting iniconfig>=1.0.1 (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama)\n",
      "  Obtaining dependency information for iniconfig>=1.0.1 from https://files.pythonhosted.org/packages/cb/b1/3846dd7f199d53cb17f49cba7e651e9ce294d8497c8c150530ed11865bb8/iniconfig-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading iniconfig-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\jooan\\appdata\\roaming\\python\\python312\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (26.0)\n",
      "Collecting pluggy<2,>=1.5 (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama)\n",
      "  Obtaining dependency information for pluggy<2,>=1.5 from https://files.pythonhosted.org/packages/54/20/4d324d65cc6d9205fabedc306948156824eb9f0ee1633355a8f7ec5c66bf/pluggy-1.6.0-py3-none-any.whl.metadata\n",
      "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\jooan\\appdata\\roaming\\python\\python312\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (2.19.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.6.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.26.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.0.3)\n",
      "Downloading llama_index_embeddings_ollama-0.8.6-py3-none-any.whl (6.3 kB)\n",
      "Downloading pytest_asyncio-1.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading pytest-9.0.2-py3-none-any.whl (374 kB)\n",
      "   ---------------------------------------- 0.0/374.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 153.6/374.8 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 374.8/374.8 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading iniconfig-2.3.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: pluggy, iniconfig, pytest, pytest-asyncio, llama-index-embeddings-ollama\n",
      "Successfully installed iniconfig-2.3.0 llama-index-embeddings-ollama-0.8.6 pluggy-1.6.0 pytest-9.0.2 pytest-asyncio-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts py.test.exe and pytest.exe are installed in 'c:\\Users\\jooan\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c3a838-0dff-45b8-95a5-052e27106d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6ec5c-1fc5-4f75-95bf-a742ba3ebb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    tokenizer_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    context_window = 8192,\n",
    "    max_new_tokens = 256,\n",
    "    generate_kwargs = {\"temperature\": 0.1, \"do_sample\": False},\n",
    "    device_map = \"auto\",\n",
    "    model_kwargs = {\"torch_dtype\": torch.float16, \"load_in_8bit\": False} \n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "Settings.embed_model = OllamaEmbedding(model_name = \"llama3\")\n",
    "\n",
    "Settings.llm = Ollama(\n",
    "    model = \"llama3\",\n",
    "    request_timeout = 360.0,\n",
    "    temperature = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a4e1273-c29e-428a-8955-e8ecf9be3d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 00:23:35,841 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 00:23:36,403 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 00:23:38,876 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Loading documents\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\".\", \n",
    "    required_exts=[\".pdf\", \".docx\"] \n",
    ")\n",
    "documents = reader.load_data()\n",
    "\n",
    "# Creating index and engine\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b679d2d-96f5-4c76-96fc-05906e25ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is difference between prompt sensitivity and model sensitivity?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d732891-0c0d-4829-937b-ad16c04855d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking Question: What is difference between prompt sensitivity and model sensitivity?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 00:24:09,954 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 00:24:31,944 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "Response:\n",
      "Prompt Sensitivity refers to testing multiple prompt templates/phrasings to assess variation in metrics across prompts. This involves examining how different prompts affect the performance of a language model.\n",
      "\n",
      "Model Sensitivity, on the other hand, involves comparing models (e.g., GPT, Llama, Deepseek) and examining where they disagree with humans and with each other. This highlights the differences in their capabilities and limitations.\n",
      "==========\n",
      "Citations:\n",
      "- Page 4: 4/13\n",
      "Why We Need a Systematic Framework\n",
      "Can an LLM do this task?\n",
      "V.S.\n",
      "How can we validate and docume...\n",
      "- Page 8: 8/13\n",
      "Step 3-Stability Evaluation:Reduce cross-prompt/model variance\n",
      "Prompt Sensitivity\n",
      "- Test multip...\n"
     ]
    }
   ],
   "source": [
    "# Input (question)\n",
    "print(f\"Asking Question: {question}\")\n",
    "response = query_engine.query(question)\n",
    "\n",
    "# Output (answer)\n",
    "print(\"\\n\" + \"=\" * 10)\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "print(\"=\" * 10)\n",
    "\n",
    "# Source\n",
    "print(\"Citations:\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"- Page {node.metadata['page_label']}: {node.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b69b822-d2f4-4dc1-8ff7-e948bba89a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 00:27:10,332 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Without RAG]:\n",
      "Cassandra Tai is a popular Australian YouTuber and beauty influencer. She was born on October 24, 1994, in Melbourne, Australia. Tai initially gained fame on YouTube by sharing makeup tutorials, product reviews, and lifestyle vlogs.\n",
      "\n",
      "Over time, she expanded her content to include skincare routines, hair care tips, and even mental health discussions. Her relatable personality, honesty, and authenticity have helped her build a massive following across various social media platforms.\n",
      "\n",
      "Cassandra Tai has collaborated with several well-known beauty brands and has been featured in prominent publications like Vogue Australia and Harper's Bazaar Australia. She is also an advocate for body positivity, self-acceptance, and mental wellness, using her platform to raise awareness about important issues.\n",
      "\n",
      "Would you like to know more about Cassandra Tai's personal life, or perhaps some of her most popular content?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 00:27:16,002 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 00:27:23,048 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Using RAG]:\n",
      "According to the provided information, Cassandra Tai is the author of a research paper titled \"Applications- GenAI vs. Human Fact-checker\" published in 2025.\n"
     ]
    }
   ],
   "source": [
    "# Without RAG\n",
    "raw_response = Settings.llm.complete(\"Who is Cassandra Tai?\")\n",
    "print(f\"[Without RAG]:\\n{raw_response}\\n\")\n",
    "\n",
    "# Using RAG\n",
    "rag_response = query_engine.query(\"Who is Cassandra Tai?\")\n",
    "print(f\"[Using RAG]:\\n{rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bf3f0e0-cc8e-41d9-9477-175c3158e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71eb801d-f7c9-4fc0-a1c4-be13e4cb04ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is difference between prompt sensitivity and model sensitivity?\n",
      "\n",
      "[score 0.3950] 8/13\n",
      "Step 3-Stability Evaluation:Reduce cross-prompt/model variance\n",
      "Prompt Sensitivity\n",
      "- Test multiple prompt templates / phrasings\n",
      "- Assess variation in metrics across prompts\n",
      "Model sensitivity\n",
      "- Compare models (GPT, Llama, Deepseek, etc\n",
      "[score 0.0754] Results: GenAI has potential but is fundamentally limited in its\n",
      "capacity to detect political content credibility\n",
      "[score 0.0730] )\n",
      "- Examine where models disagree with humans and with each other 9/13\n",
      "Step 4-Explainability & Oversight: Automated systems require oversight\n",
      "Use model-generated rationales as auditable artifacts\n",
      "Audit for:\n",
      "- Logical coherence and conceptual validity\n",
      "- Biases, hallucinations, ethical red flags\n",
      "Two-way humanâ€“AI collaboration\n",
      "- LLM rationales expand human awareness\n",
      "- Human experts correct, constrain, and document model behavior 10/13\n",
      "Step 5-Uncertainty & Error Correction:Acknowledging AI uncertainties\n",
      "Misclassification bias can distort regression and descriptive\n",
      "analyses\n",
      "Treat AI labels as noisy measurements, not ground truth\n",
      "Methods:\n",
      "- Design-based Supervised Learning (DSL): gold-standard subsample + inclusion\n",
      "probabilities (Egami et al\n"
     ]
    }
   ],
   "source": [
    "### Pure python retrieval\n",
    "doc_text = \" \".join([doc.text for doc in documents])\n",
    "corpus = [sent.strip() for sent in doc_text.split('.') if len(sent) > 20]\n",
    "\n",
    "# Question\n",
    "query = \"What is difference between prompt sensitivity and model sensitivity?\"\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus + [query])\n",
    "\n",
    "# Calculating cosine similarity\n",
    "similarities = cosine_similarity(X[-1], X[:-1])\n",
    "\n",
    "# Top 3 sentences\n",
    "top_k = 3\n",
    "top_indices = np.argsort(similarities[0])[-top_k:][::-1]\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "for idx in top_indices:\n",
    "    score = similarities[0][idx]\n",
    "    print(f\"[score {score:.4f}] {corpus[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc5c33c0-fc5e-4116-9205-cd91f28fa7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 00:28:05,130 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 00:28:17,959 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Answer (Top-k=1)]: The Conception Task Prompts Gold section outlines a framework for creating high-quality labels or clusters as gold standards. This involves two main approaches: zero-shot and few-shot learning. Zero-shot learning aims to recover gold labels without any prior training data, focusing on reliability metrics such as Cohen's kappa and Krippendorff's alpha. Few-shot learning, on the other hand, involves fine-tuning a machine learning system using limited labeled data, with validity metrics like precision, recall, and F1-score.\n",
      "\n",
      "The Like-human section explores alternative methods for creating gold labels, including human subjects (silicon participants) who take surveys, play games, or engage in other activities to simulate human behavior. Validity metrics for this approach include mean/standard deviation relative to human baseline data and human-ness factors like fluency, cohesiveness, objectivity, readability, and more.\n",
      "\n",
      "Overall, the document presents a comprehensive framework for creating high-quality gold labels, emphasizing both machine learning-based and human-centered approaches.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 00:28:23,897 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 00:28:37,696 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Answer (Top-k=5)]: The importance of acknowledging AI uncertainties in machine learning systems is highlighted. The framework presented emphasizes the need for a systematic approach to validating and documenting LLM use to ensure credible inferences.\n",
      "\n",
      "The methodology involves designing a dual-track system that evaluates both reliability and validity. This includes metrics such as Cohen's kappa, Krippendorff's alpha, precision, recall, F1 score, AUC, MCC, Silhouette coefficient, pair comparison, and mean/SD relative to human baseline data.\n",
      "\n",
      "The framework also acknowledges the potential for misclassification bias in regression and descriptive analyses when treating AI labels as ground truth. To address this, methods such as Design-based Supervised Learning (DSL) and Misclassification / maximum-likelihood adjustment (MLA) can be employed.\n",
      "\n",
      "In terms of applications, the framework is demonstrated through a case study comparing GenAI with human fact-checkers. The results show that GenAI can be used to validate and document LLM use in various contexts.\n",
      "\n",
      "The conclusion emphasizes the importance of acknowledging AI uncertainties and developing systematic frameworks for evaluating and documenting LLM use. By doing so, researchers can ensure credible inferences and improve the overall reliability and validity of their findings.\n"
     ]
    }
   ],
   "source": [
    "### Parameter tuning \n",
    "\n",
    "# top-k = 1\n",
    "engine_k1 = index.as_query_engine(similarity_top_k=1)\n",
    "response_k1 = engine_k1.query(\"Summarize the entire document including methodology, results, and conclusion.\")\n",
    "print(f\"[Answer (Top-k=1)]: {response_k1}\\n\")\n",
    "\n",
    "# top-k = 5\n",
    "engine_k5 = index.as_query_engine(similarity_top_k=5)\n",
    "response_k5 = engine_k5.query(\"Summarize the entire document including methodology, results, and conclusion.\")\n",
    "print(f\"[Answer (Top-k=5)]: {response_k5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f7b5009-83de-4101-adb2-f338a46aed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 00:29:12,687 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector dimension of question 'Social Science': 4096\n",
      "First 10 elements of the vector: [-0.016794328, -0.006202411, 0.017395807, 0.0011353239, -0.0016906768, 0.01175532, -0.019479135, 0.017811432, -0.004520285, 0.017569028]\n"
     ]
    }
   ],
   "source": [
    "### Vector Embeddings Visualization\n",
    "\n",
    "query_str = \"Social Science\"\n",
    "query_embedding = Settings.embed_model.get_query_embedding(query_str)\n",
    "\n",
    "print(f\"Vector dimension of question '{query_str}': {len(query_embedding)}\")\n",
    "print(f\"First 10 elements of the vector: {query_embedding[:10]}\")\n",
    "\n",
    "# These numbers collectively represent the semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52638f47-4324-449c-9554-a3da123d88bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd31882-d17b-4c1e-8fc8-1ce532905db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
