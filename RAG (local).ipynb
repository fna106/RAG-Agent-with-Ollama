{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec052c0-3bb2-466b-a7e0-c74460e858e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport torch\\nfrom huggingface_hub import login\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5d312f-b1b0-438a-8755-20932c882c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Setting working directory\\nwork_dir = \"/storage/work/~~~\" \\nos.environ[\\'HF_HOME\\'] = f\"{work_dir}/.cache/huggingface\"\\nos.environ[\\'TRANSFORMERS_CACHE\\'] = f\"{work_dir}/.cache/huggingface\"\\n\\n# Huggingface token\\nhf_token = \"hf_~~~\" \\nlogin(token = hf_token)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Setting working directory\n",
    "work_dir = \"/storage/work/~~~\" \n",
    "os.environ['HF_HOME'] = f\"{work_dir}/.cache/huggingface\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = f\"{work_dir}/.cache/huggingface\"\n",
    "\n",
    "# Huggingface token\n",
    "hf_token = \"hf_~~~\" \n",
    "login(token = hf_token)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40761b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-embeddings-ollama in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.6)\n",
      "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-embeddings-ollama) (0.14.13)\n",
      "Requirement already satisfied: ollama>=0.3.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-embeddings-ollama) (0.6.1)\n",
      "Requirement already satisfied: pytest-asyncio>=0.23.8 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-embeddings-ollama) (1.3.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.13.3)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.22.1)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.3.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2026.1.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.13.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\jooan\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.6.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.4.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (12.1.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\jooan\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (4.5.1)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.12.5)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (80.10.2)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.0.46)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.17.3)\n",
      "Requirement already satisfied: pytest<10,>=8.2 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (9.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.22.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.15.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.1.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.16.0)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2026.1.15)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (0.4.2)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\jooan\\appdata\\roaming\\python\\python312\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (0.4.6)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (2.3.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\jooan\\appdata\\roaming\\python\\python312\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (26.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\jooan\\appdata\\roaming\\python\\python312\\site-packages (from pytest<10,>=8.2->pytest-asyncio>=0.23.8->llama-index-embeddings-ollama) (2.19.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (2.6.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.26.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jooan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-ollama) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c3a838-0dff-45b8-95a5-052e27106d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b6ec5c-1fc5-4f75-95bf-a742ba3ebb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    tokenizer_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    context_window = 8192,\n",
    "    max_new_tokens = 256,\n",
    "    generate_kwargs = {\"temperature\": 0.1, \"do_sample\": False},\n",
    "    device_map = \"auto\",\n",
    "    model_kwargs = {\"torch_dtype\": torch.float16, \"load_in_8bit\": False} \n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "Settings.embed_model = OllamaEmbedding(model_name = \"llama3\")\n",
    "\n",
    "Settings.llm = Ollama(\n",
    "    model = \"llama3\",\n",
    "    request_timeout = 360.0,\n",
    "    temperature = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a4e1273-c29e-428a-8955-e8ecf9be3d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 01:41:25,625 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 01:41:26,185 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 01:41:28,662 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Loading documents\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\".\", \n",
    "    required_exts=[\".pdf\", \".docx\"] \n",
    ")\n",
    "documents = reader.load_data()\n",
    "\n",
    "# Creating index and engine\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b679d2d-96f5-4c76-96fc-05906e25ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is difference between prompt sensitivity and model sensitivity?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d732891-0c0d-4829-937b-ad16c04855d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 01:41:28,867 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is difference between prompt sensitivity and model sensitivity?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 01:41:40,550 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Chunk #1 | Score: 0.6559 | Source: LLM_SSRI.pdf (Page 4)\n",
      "Content: \"4/13 Why We Need a Systematic Framework Can an LLM do this task? V.S. How can we validate and document LLM use so that our inferences remain credible?...\"\n",
      "--------------------------------------------------\n",
      "Chunk #2 | Score: 0.5936 | Source: LLM_SSRI.pdf (Page 8)\n",
      "Content: \"8/13 Step 3-Stability Evaluation:Reduce cross-prompt/model variance Prompt Sensitivity - Test multiple prompt templates / phrasings - Assess variation in metrics across prompts Model sensitivity - Com...\"\n",
      "--------------------------------------------------\n",
      "RESPONSE:\n",
      "Prompt Sensitivity refers to testing multiple prompt templates/phrasings and assessing variation in metrics across prompts. This involves evaluating how different prompts affect the performance of a language model.\n",
      "\n",
      "Model Sensitivity, on the other hand, involves comparing models (e.g., GPT, Llama, Deepseek) and examining where they disagree with humans and with each other. This focuses on understanding the differences between various language models themselves.\n",
      "--------------------------------------------------\n",
      "Citations:\n",
      "- Page 4: 4/13\n",
      "Why We Need a Systematic Framework\n",
      "Can an LLM do this task?\n",
      "V.S.\n",
      "How can we validate and docume...\n",
      "- Page 8: 8/13\n",
      "Step 3-Stability Evaluation:Reduce cross-prompt/model variance\n",
      "Prompt Sensitivity\n",
      "- Test multip...\n"
     ]
    }
   ],
   "source": [
    "# Input (question)\n",
    "print(f\"Question: {question}\\n\")\n",
    "response = query_engine.query(question)\n",
    "\n",
    "## Retrieved snippets\n",
    "print(\"-\" * 50)\n",
    "for i, node in enumerate(response.source_nodes, 1):\n",
    "    # chunk ID\n",
    "    page = node.metadata.get('page_label', 'N/A')\n",
    "    filename = node.metadata.get('file_name', 'N/A')\n",
    "    score = node.score if node.score else 0.0\n",
    "    \n",
    "    # snippet text\n",
    "    content_preview = node.text[:200].replace('\\n', ' ') + \"...\"\n",
    "    \n",
    "    print(f\"Chunk #{i} | Score: {score:.4f} | Source: {filename} (Page {page})\")\n",
    "    print(f\"Content: \\\"{content_preview}\\\"\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Output (answer)\n",
    "print(\"RESPONSE:\")\n",
    "print(response)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Source\n",
    "print(\"Citations:\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"- Page {node.metadata['page_label']}: {node.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b69b822-d2f4-4dc1-8ff7-e948bba89a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 01:45:23,735 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Without RAG]:\n",
      "Cassandra Tai is a popular Australian YouTuber and beauty influencer. She was born on August 24, 1994, in Melbourne, Australia. Tai initially gained fame on YouTube by sharing her makeup tutorials, product reviews, and lifestyle vlogs.\n",
      "\n",
      "Over time, she expanded her content to include skincare routines, hair care tips, and even cooking recipes. Her relatable personality, honesty, and authenticity have made her a beloved figure among her fans, who affectionately call themselves \"Taisies.\"\n",
      "\n",
      "Cassandra Tai has collaborated with various beauty brands and has been featured in several publications, including Australian Vogue and Harper's Bazaar. She has also spoken at events like the Melbourne Fashion Week and has partnered with organizations that support mental health awareness.\n",
      "\n",
      "With over 2 million subscribers on YouTube and millions of views across her social media platforms, Cassandra Tai is a prominent figure in the beauty and lifestyle influencer space.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 01:45:33,059 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 01:45:44,311 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Using RAG]:\n",
      "According to the provided information, Cassandra Tai is the author of a research paper titled \"Applications- GenAI vs. Human Fact-checker\" published in 2025.\n"
     ]
    }
   ],
   "source": [
    "# Without RAG\n",
    "raw_response = Settings.llm.complete(\"Who is Cassandra Tai?\")\n",
    "print(f\"[Without RAG]:\\n{raw_response}\\n\")\n",
    "\n",
    "# Using RAG\n",
    "rag_response = query_engine.query(\"Who is Cassandra Tai?\")\n",
    "print(f\"[Using RAG]:\\n{rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf3f0e0-cc8e-41d9-9477-175c3158e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb801d-f7c9-4fc0-a1c4-be13e4cb04ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is difference between prompt sensitivity and model sensitivity?\n",
      "\n",
      "[score 0.3950] 8/13\n",
      "Step 3-Stability Evaluation:Reduce cross-prompt/model variance\n",
      "Prompt Sensitivity\n",
      "- Test multiple prompt templates / phrasings\n",
      "- Assess variation in metrics across prompts\n",
      "Model sensitivity\n",
      "- Compare models (GPT, Llama, Deepseek, etc\n",
      "[score 0.0754] Results: GenAI has potential but is fundamentally limited in its\n",
      "capacity to detect political content credibility\n",
      "[score 0.0730] )\n",
      "- Examine where models disagree with humans and with each other 9/13\n",
      "Step 4-Explainability & Oversight: Automated systems require oversight\n",
      "Use model-generated rationales as auditable artifacts\n",
      "Audit for:\n",
      "- Logical coherence and conceptual validity\n",
      "- Biases, hallucinations, ethical red flags\n",
      "Two-way humanâ€“AI collaboration\n",
      "- LLM rationales expand human awareness\n",
      "- Human experts correct, constrain, and document model behavior 10/13\n",
      "Step 5-Uncertainty & Error Correction:Acknowledging AI uncertainties\n",
      "Misclassification bias can distort regression and descriptive\n",
      "analyses\n",
      "Treat AI labels as noisy measurements, not ground truth\n",
      "Methods:\n",
      "- Design-based Supervised Learning (DSL): gold-standard subsample + inclusion\n",
      "probabilities (Egami et al\n"
     ]
    }
   ],
   "source": [
    "### Pure python retrieval\n",
    "doc_text = \" \".join([doc.text for doc in documents])\n",
    "corpus = [sent.strip() for sent in doc_text.split('.') if len(sent) > 20]\n",
    "\n",
    "# Question = \"What is difference between prompt sensitivity and model sensitivity?\"\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus + [question])\n",
    "\n",
    "# Cosine similarity\n",
    "similarities = cosine_similarity(X[-1], X[:-1])\n",
    "\n",
    "# Top 3 sentences\n",
    "top_k = 3\n",
    "top_indices = np.argsort(similarities[0])[-top_k:][::-1]\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "for idx in top_indices:\n",
    "    score = similarities[0][idx]\n",
    "    print(f\"[score {score:.4f}] {corpus[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c33c0-fc5e-4116-9205-cd91f28fa7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 01:59:37,472 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 01:59:57,629 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Answer (Top-k=1)]: The Conception Task Prompts Gold section outlines a framework for creating high-quality labels or clusters as gold standards. This involves two main approaches: zero-shot learning and few-shot learning with fine-tuning.\n",
      "\n",
      "In the zero-shot approach, machine learning systems can recover gold labels without any prior training data. The focus is on reliability metrics such as Cohen's kappa, Krippendorff's alpha, precision, recall, F1 score, AUC, and Matthews Correlation Coefficient (MCC).\n",
      "\n",
      "The few-shot approach involves fine-tuning the machine learning system with a small amount of labeled data. In this case, the focus shifts to validity metrics like precision, recall, F1 score, and AUC.\n",
      "\n",
      "A third approach is to create clusters of materials without prior schema using zero-shot learning. This method relies on reliability metrics such as Silhouette coefficient, pair comparison/adjusted normalized mutual information, etc.\n",
      "\n",
      "The Like-human subject section discusses a human-centered approach where a machine or silicon participant takes the place of a human subject. The focus is on validity metrics like mean/SD relative to human baseline data and human-ness factors like fluency, cohesiveness, objectivity, readability, etc.\n",
      "\n",
      "In summary, this document outlines various approaches for creating high-quality labels or clusters as gold standards, including zero-shot learning, few-shot learning with fine-tuning, and a human-centered approach. The focus is on reliability and validity metrics to ensure the quality of the generated labels or clusters.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 02:00:07,370 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 02:00:25,003 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Answer (Top-k=5)]: The importance of acknowledging AI uncertainties in machine learning systems is highlighted. The framework presented emphasizes the need for a systematic approach to validating and documenting LLM use to ensure credible inferences.\n",
      "\n",
      "The methodology involves designing a dual-track system with reliability and validity tracks. Reliability track metrics include Cohen's kappa, Krippendorff's alpha, and other measures of inter-rater agreement. Validity track metrics encompass precision, recall, F1 score, AUC, MCC, and other measures of accuracy. The silicon participant track focuses on mean/SD relative to human baseline data and human-ness metrics like fluency, cohesiveness, objectivity, readability, etc.\n",
      "\n",
      "The results demonstrate the effectiveness of this framework in evaluating LLMs' role in various tasks. By acknowledging AI uncertainties and treating labels as noisy measurements, researchers can develop more robust methods for regression and descriptive analyses.\n",
      "\n",
      "In conclusion, the proposed framework provides a comprehensive approach to validating and documenting LLM use, ensuring credible inferences and promoting reliable machine learning systems.\n"
     ]
    }
   ],
   "source": [
    "### Parameter tuning \n",
    "\n",
    "question2 = \"Summarize the entire document including methodology, results, and conclusion.\"\n",
    "\n",
    "# top-k = 1\n",
    "engine_k1 = index.as_query_engine(similarity_top_k=1)\n",
    "response_k1 = engine_k1.query(question2)\n",
    "print(f\"[Answer (Top-k=1)]: {response_k1}\\n\")\n",
    "\n",
    "# top-k = 5\n",
    "engine_k5 = index.as_query_engine(similarity_top_k=5)\n",
    "response_k5 = engine_k5.query(question2)\n",
    "print(f\"[Answer (Top-k=5)]: {response_k5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f7b5009-83de-4101-adb2-f338a46aed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 01:43:16,178 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector dimension of question 'Social Science': 4096\n",
      "First 10 elements of the vector: [-0.016794328, -0.006202411, 0.017395807, 0.0011353239, -0.0016906768, 0.01175532, -0.019479135, 0.017811432, -0.004520285, 0.017569028]\n"
     ]
    }
   ],
   "source": [
    "### Vector Embeddings Visualization\n",
    "\n",
    "query_str = \"Social Science\"\n",
    "query_embedding = Settings.embed_model.get_query_embedding(query_str)\n",
    "\n",
    "print(f\"Vector dimension of question '{query_str}': {len(query_embedding)}\")\n",
    "print(f\"First 10 elements of the vector: {query_embedding[:10]}\")\n",
    "\n",
    "# These numbers collectively represent the semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52638f47-4324-449c-9554-a3da123d88bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd31882-d17b-4c1e-8fc8-1ce532905db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
